{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4168f35a-ecbc-4369-a375-3d41b7f28812",
   "metadata": {},
   "source": [
    "### Tests\n",
    "Can the LLM write decent unit tests for our functions?\n",
    "\n",
    "There's only two functions and Ive pasted them into a py file for easy access. Lets import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1378d829-9298-44be-9c4f-7cab5f4ddedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_functions import objective, app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e027b-6ffa-4082-82eb-a2d9a17f198b",
   "metadata": {},
   "source": [
    "I'll now paste the functions into LLM and ask it to create unit tests for them.\n",
    "\n",
    "**Prompt:** use pytest to test this function\n",
    "\n",
    "**Follow up Prompt:** The function is an objective for the Optuna library to optimize. It does not have any inputs. It creates and xgboost model and then returns the accuracy of the model. The function is fed into Optuna's study.optimize().\n",
    "\n",
    "**Result:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc38061a-7237-4731-9118-1425ce48efa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated: number (1556878068.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 28\u001b[1;36m\u001b[0m\n\u001b[1;33m    number=0\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword argument repeated: number\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import unittest.mock as mock\n",
    "import optuna\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test_objective():\n",
    "   # Load a sample dataset\n",
    "   iris = load_iris()\n",
    "   X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=1)\n",
    "\n",
    "   # Mock the trial object\n",
    "   trial = optuna.trial.Trial(\n",
    "       number=0,\n",
    "       state=optuna.trial.TrialState.RUNNING,\n",
    "       value=None,\n",
    "       datetime_start=None,\n",
    "       datetime_complete=None,\n",
    "       params={},\n",
    "       distributions={},\n",
    "       user_attrs={},\n",
    "       system_attrs={},\n",
    "       intermediate_values={},\n",
    "       trial_id=0,\n",
    "       study_id=0,\n",
    "       number=0\n",
    "   )\n",
    "\n",
    "   # Define fixed hyperparameters\n",
    "   trial.suggest_int = lambda name, low, high: 100 # n_estimators\n",
    "   trial.suggest_int = lambda name, low, high: 1 # max_depth\n",
    "   trial.suggest_float = lambda name, low, high, log: 0.5 # min_child_weight, learning_rate, gamma, reg_alpha, reg_lambda\n",
    "   trial.suggest_float = lambda name, low, high: 0.5 # subsample, colsample_bytree\n",
    "\n",
    "   # Call the function\n",
    "   accuracy = objective(trial)\n",
    "\n",
    "   # Check the accuracy\n",
    "   xgb_model = XGBClassifier(n_estimators=100, max_depth=1, min_child_weight=0.5, learning_rate=0.5, gamma=0.5, subsample=0.5, colsample_bytree=0.5, reg_alpha=0.5, reg_lambda=0.5, random_state=99)\n",
    "   xgb_model.fit(X_train, y_train)\n",
    "   y_pred = xgb_model.predict(X_test)\n",
    "   expected_accuracy = accuracy_score(y_test, y_pred)\n",
    "   assert accuracy == expected_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e78df-ca73-4e08-932f-659ed029a60a",
   "metadata": {},
   "source": [
    "Oh dear, thats no good. Many things are wrong here.\n",
    "\n",
    "it imported unittest.mock and then never used it (the commentary said it had to mock the data).\n",
    "\n",
    "it did not need to actually import pytest.\n",
    "\n",
    "there is a repeated keyword argument.\n",
    "\n",
    "the keyword arguments are all completely wrong and dont even exist in that class.\n",
    "\n",
    "its trying to create fixed hyper parameters but isnt useing the class optuna.trial.FixedTrial.\n",
    "\n",
    "it then tries to compare the objective accuracy to a newly fitted models accuracy. They will certainly be different since the former is based on cross validation and the latter is not.\n",
    "\n",
    "I'd say, this is unsalvageable. I tried telling the model that the trial object took different keywords. It understood and revised the test but it was still nonesense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267bbb3-4c7a-49b3-8499-1f115856fb28",
   "metadata": {},
   "source": [
    "How about our FastAPI app?\n",
    "\n",
    "**Prompt:** I have a function that uses pydantic's Basemodel and FastAPI to return a prediction for an xgboost model. write a test for this functions using py test.\n",
    "\n",
    "**Follow up Prompt:** The data will be in json format. The Data model consistes of our ml model's features and dtypes. The xgboost model is a XGBClassifier object that is already in memory (its just called \"model\")\n",
    "\n",
    "**Result:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74107a0-9e5f-4cf4-9859-da8b036b4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('xgb_model.pkl', 'rb') as file:\n",
    "   model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc3d52e-52b4-43fa-a0ca-7e8639faac32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'httpx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestClient\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\xgb-llm\\Lib\\site-packages\\fastapi\\testclient.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstarlette\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtestclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestClient \u001b[38;5;28;01mas\u001b[39;00m TestClient  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\xgb-llm\\Lib\\site-packages\\starlette\\testclient.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01manyio\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01manyio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrom_thread\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manyio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreams\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstapled\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StapledObjectStream\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstarlette\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_async_callable\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'httpx'"
     ]
    }
   ],
   "source": [
    "from fastapi.testclient import TestClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2238ec-353a-4826-8b69-43a36f84fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "#from main import app, model # assuming your FastAPI app and model are defined in a file named main.py\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_predict():\n",
    "   # Define test data\n",
    "   test_data = {\n",
    "       \"tenure\": 12,\n",
    "       \"MonthlyCharges\": 20,\n",
    "       \"TotalCharges\": 240,\n",
    "       \"TotalServices\": 3,\n",
    "       \"gender_Male\": 1,\n",
    "       \"InternetService_Fiber_optic\": 0,\n",
    "       \"InternetService_No\": 1,\n",
    "       \"Contract_One_year\": 0,\n",
    "       \"Contract_Two_year\": 1,\n",
    "       \"PaymentMethod_Credit_card_automatic\": 0,\n",
    "       \"PaymentMethod_Electronic_check\": 1,\n",
    "       \"PaymentMethod_Mailed_check\": 0,\n",
    "       \"SeniorCitizen\": 0,\n",
    "       \"Partner\": 1,\n",
    "       \"Dependents\": 0,\n",
    "       \"PhoneService\": 1,\n",
    "       \"MultipleLines\": 0,\n",
    "       \"OnlineSecurity\": 0,\n",
    "       \"OnlineBackup\": 0,\n",
    "       \"DeviceProtection\": 1,\n",
    "       \"TechSupport\": 0,\n",
    "       \"StreamingTV\": 0,\n",
    "       \"StreamingMovies\": 0,\n",
    "       \"PaperlessBilling\": 1,\n",
    "   }\n",
    "\n",
    "   # Send a POST request to the /predict endpoint\n",
    "   response = client.post(\"/predict\", json=test_data)\n",
    "\n",
    "   # Check the status code\n",
    "   assert response.status_code == 200\n",
    "\n",
    "   # Check the returned prediction\n",
    "   assert \"prediction\" in response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11293566-9249-4768-85c1-60a81dad5878",
   "metadata": {},
   "source": [
    "Alright. This seems to align with the official documentation: https://fastapi.tiangolo.com/tutorial/testing/\n",
    "\n",
    "it got the variable names partially wrong though (remember that i used aliases in my app). easy to change.\n",
    "\n",
    "I suppose this is satisfactory. There could be other tests which check the actual returned values. Though this wouldnt sctrictly be testing app functionality which is what we had in mind with our prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d334b-ae96-4aed-b9fe-23c60951ad82",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Half marks for the LLM here. Took care of FastAPI app no problem but was fairly hopeless at the Optuna objective function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
