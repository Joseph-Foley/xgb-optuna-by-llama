{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4168f35a-ecbc-4369-a375-3d41b7f28812",
   "metadata": {},
   "source": [
    "### Tests\n",
    "Can the LLM write decent unit tests for our functions?\n",
    "\n",
    "There's only two functions and Ive pasted them into a py file for easy access. Lets import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1378d829-9298-44be-9c4f-7cab5f4ddedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_functions import objective, app\n",
    "from project_functions import objective, app\n",
    "from fastapi.testclient import TestClient\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e027b-6ffa-4082-82eb-a2d9a17f198b",
   "metadata": {},
   "source": [
    "I'll now paste the functions into LLM and ask it to create unit tests for them.\n",
    "\n",
    "**Prompt:** use pytest to test this function\n",
    "\n",
    "**Follow up Prompt:** The function is an objective for the Optuna library to optimize. It does not have any inputs. It creates and xgboost model and then returns the accuracy of the model. The function is fed into Optuna's study.optimize().\n",
    "\n",
    "**Result:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46c938-80d4-4fb1-ae73-fd74da7e09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_objective():\n",
    "    accuracy = round(\n",
    "        objective(\n",
    "            optuna.trial.FixedTrial({\"n_estimators\": 800, \"max_depth\": 2, \"min_child_weight\": 10,\n",
    "                                     \"learning_rate\": 0.03, \"gamma\": 0.01, \"subsample\": 0.7,\n",
    "                                     \"colsample_bytree\": 0.8, \"reg_alpha\": 0.01, \"reg_lambda\": 0.05})\n",
    "                    ), 3)\n",
    "\n",
    "    assert accuracy == 0.797"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e78df-ca73-4e08-932f-659ed029a60a",
   "metadata": {},
   "source": [
    "I completely rewrote this as the LLM's output was just a mess.\n",
    "\n",
    "Optuna documentation gives good info on how to test objective functions: https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-test-my-objective-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267bbb3-4c7a-49b3-8499-1f115856fb28",
   "metadata": {},
   "source": [
    "How about our FastAPI app?\n",
    "\n",
    "**Prompt:** I have a function that uses pydantic's Basemodel and FastAPI to return a prediction for an xgboost model. write a test for this functions using py test.\n",
    "\n",
    "**Follow up Prompt:** The data will be in json format. The Data model consistes of our ml model's features and dtypes. The xgboost model is a XGBClassifier object that is already in memory (its just called \"model\")\n",
    "\n",
    "**Result:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2238ec-353a-4826-8b69-43a36f84fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = TestClient(app)\n",
    "def test_predict():\n",
    "    # Define test data\n",
    "    test_data = {\n",
    "       \"tenure\": 12,\n",
    "       \"MonthlyCharges\": 20,\n",
    "       \"TotalCharges\": 240,\n",
    "       \"TotalServices\": 3,\n",
    "       \"gender_Male\": 1,\n",
    "       \"InternetService_Fiber optic\": 0,\n",
    "       \"InternetService_No\": 1,\n",
    "       \"Contract_One year\": 0,\n",
    "       \"Contract_Two year\": 1,\n",
    "       \"PaymentMethod_Credit card (automatic)\": 0,\n",
    "       \"PaymentMethod_Electronic check\": 1,\n",
    "       \"PaymentMethod_Mailed check\": 0,\n",
    "       \"SeniorCitizen\": 0,\n",
    "       \"Partner\": 1,\n",
    "       \"Dependents\": 0,\n",
    "       \"PhoneService\": 1,\n",
    "       \"MultipleLines\": 0,\n",
    "       \"OnlineSecurity\": 0,\n",
    "       \"OnlineBackup\": 0,\n",
    "       \"DeviceProtection\": 1,\n",
    "       \"TechSupport\": 0,\n",
    "       \"StreamingTV\": 0,\n",
    "       \"StreamingMovies\": 0,\n",
    "       \"PaperlessBilling\": 1,\n",
    "    }\n",
    "    \n",
    "    # Send a POST request to the /predict endpoint\n",
    "    response = client.post(\"/predict\", json=test_data)\n",
    "    \n",
    "    # Check the status code\n",
    "    assert response.status_code == 200\n",
    "    \n",
    "    # Check the returned prediction\n",
    "    assert \"prediction\" in response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11293566-9249-4768-85c1-60a81dad5878",
   "metadata": {},
   "source": [
    "Alright. This seems to align with the official documentation: https://fastapi.tiangolo.com/tutorial/testing/\n",
    "\n",
    "it got the variable names partially wrong though (remember that i used aliases in my app). easy to change.\n",
    "\n",
    "I suppose this is satisfactory. There could be other tests which check the actual returned values. Though this wouldnt sctrictly be testing app functionality which is what we had in mind with our prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d334b-ae96-4aed-b9fe-23c60951ad82",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Half marks for the LLM here. Took care of FastAPI app no problem but was fairly hopeless at the Optuna objective function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
